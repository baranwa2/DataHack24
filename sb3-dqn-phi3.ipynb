{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers huggingface_hub[cli] bitsandbytes accelerate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install environment and agent\n!pip install highway-env\n# TODO: we use the bleeding edge version because the current stable version does not support the latest gym>=0.21 versions. Revert back to stable at the next SB3 release.\n!pip install git+https://github.com/DLR-RM/stable-baselines3\n\n# Environment\nimport gymnasium as gym\nimport highway_env\n\n# Agent\nfrom stable_baselines3 import DQN\n\n# Visualization utils\n%load_ext tensorboard\nimport sys\nfrom tqdm.notebook import trange\n!pip install tensorboardx gym pyvirtualdisplay\n!apt-get install -y xvfb ffmpeg\n!git clone https://github.com/Farama-Foundation/HighwayEnv.git 2> /dev/null\nsys.path.insert(0, '/kaggle/working/HighwayEnv/scripts/')\nfrom utils import record_videos, show_videos","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\n\nconfig = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\n\nllm_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3-mini-4k-instruct\",\n    device_map=\"cuda\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True,\n    quantization_config = config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def llm_action(prompt1, assist1, prompt2, last_act='FASTER'):\n    messages = [{\"role\": \"user\", \"content\": prompt1},\n           {\"role\": \"assistant\", \"content\": assist1},\n           {\"role\": \"user\", \"content\": prompt2}]\n\n    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n\n    output = llm_model.generate(model_inputs, max_new_tokens=2000, do_sample=True)\n\n    decoded_output = tokenizer.batch_decode(output[:,model_inputs.size(1):], skip_special_tokens=True)\n    \n    try:\n        action = decoded_output[0].strip().split('Final decision: ')[1].strip().split('\\'')[0]\n    except:\n        action = last_act\n    \n    return action","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3 import DQN\nimport pprint\nfrom matplotlib import pyplot as plt\nimport numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"action_dict = {\n    0: 'LANE_LEFT',\n    1: 'IDLE',\n    2: 'LANE_RIGHT',\n    3: 'FASTER',\n    4: 'SLOWER'\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyHighwayEnv(gym.Env):\n    def __init__(self, vehicleCount=10):\n        super(MyHighwayEnv, self).__init__()\n        # base setting\n        self.vehicleCount = vehicleCount\n        self.prev_action  = 'FASTER'\n        \n        # environment setting\n        self.config = {\n            \"observation\": {\n                \"type\": \"Kinematics\",\n                \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n                \"absolute\": True,\n                \"normalize\": False,\n                \"vehicles_count\": vehicleCount,\n                \"see_behind\": True,\n            },\n            \"action\": {\n                \"type\": \"DiscreteMetaAction\",\n                \"target_speeds\": np.linspace(0, 32, 9),\n            },\n            \"duration\": 40,\n            \"vehicles_density\": 2,\n            \"show_trajectories\": True,\n            \"render_agent\": True,\n        }\n        self.env = gym.make(\"highway-fast-v0\")\n        self.env.configure(self.config)\n        self.action_space = self.env.action_space\n        self.observation_space = gym.spaces.Box(\n            low=-np.inf,high=np.inf,shape=(10,5),dtype=np.float32\n        )\n\n    def find_smallest_positive(self, arr):\n        smallest_positive = float('inf')\n        index = -1\n\n        for i, value in enumerate(arr):\n            if 0 < value < smallest_positive:\n                smallest_positive = value\n                index = i\n\n        return smallest_positive, index\n\n    def prompt_design(self, obs_):\n\n        prompt1 = 'You are a smart driving assistant. You, the \\'ego\\' car, are now driving on a highway. You need to recommend ONLY ONE best action among the following set of actions based on the current scenario: \\n \\\n        \\t1. IDLE -- maintain the current speed in the current lane \\n \\\n        \\t2. FASTER -- accelerate the ego vehicle \\n \\\n        \\t3. SLOWER -- decelerate the ego vehicle \\n \\\n        \\t4. LANE_LEFT -- change to the adjacent left lane \\n \\\n        \\t5. LANE_RIGHT -- change to the adjacent right lane\\n'\n\n        assist1 = 'Understood. Please provide the current scenario or conditions, such as traffic density, speed of surrounding vehicles, your current speed, and any other relevant information, so I can recommend the best action.'\n\n        prompt2 = 'Here is the current scenario:\\n \\\n        There are four lanes on the highway: Lane-1 (left most), Lane-2, Lane-3, Lane-4 (right most). \\n\\n'\n\n        x, y, vx, vy = obs_[:,1], obs_[:,2], obs_[:,3], obs_[:,4]\n\n        ego_x, ego_y   = x[0], y[0]\n        ego_vx, ego_vy = vx[0], vy[0]\n\n        veh_x, veh_y   = x[1:] - ego_x, y[1:] - ego_y\n        veh_vx, veh_vy = vx[1:], vy[1:]\n\n        lanes          = y//4+1\n        ego_lane       = lanes[0]\n        veh_lanes      = lanes[1:]\n\n        if ego_lane == 1:\n            ego_left_lane  = 'Left lane: Not available\\n'\n            ego_right_lane = 'Right lane: Lane-' + str(ego_lane+1) + '\\n'\n        elif ego_lane == 4:\n            ego_left_lane  = 'Left lane: Lane-' + str(ego_lane-1) + '\\n'\n            ego_right_lane = 'Right lane: Not available\\n'\n        else:\n            ego_left_lane  = 'Left lane: Lane-' + str(ego_lane-1) + '\\n'\n            ego_right_lane = 'Right lane: Lane-' + str(ego_lane+1) + '\\n'\n\n        prompt2 += 'Ego vehicle:\\n \\\n        \\tCurrent lane: Lane-' + str(ego_lane) + '\\n' + '\\t' + ego_left_lane + '\\t' + ego_right_lane + '\\tCurrent speed: ' + str(ego_vx) + ' m/s \\n\\n'\n\n        lane_info = 'Lane info:\\n'\n        for i in range(4):\n            inds     = np.where(veh_lanes == i+1)[0]\n            num_v    = len(inds)\n            if num_v > 0:\n                val, ind = self.find_smallest_positive(veh_x[inds])\n                true_ind = inds[ind]\n                lane_info += '\\tLane-' + str(i+1) + ': There are ' + str(num_v) + ' vehicle(s) in this lane ahead of ego vehicle, closest being ' + str(veh_x[true_ind]) + ' m ahead traveling at ' + str(veh_vx[true_ind]) + ' m\\/s. \\n'\n            else:\n                lane_info += '\\tLane-' + str(i+1) + ' No other vehicle ahead of ego vehicle.\\n'\n\n        prompt2 += lane_info\n\n        att_info = '\\nAttention points:\\n \\\n        \\t1. SLOWER has least priority and should be used only when no other action is safe.\\n \\\n        \\t2. DO NOT change lanes frequently.\\n \\\n        \\t3. Safety is priority, but do not forget efficiency.\\n \\\n        \\t4. Your suggested action has to be one from one of the above five listed actions - IDLE, SLOWER, FASTER, LANE_LEFT, LANE_RIGHT. \\n \\\n        Your last action was ' + self.prev_action + '. Please recommend action for the current scenario ONLY in the format \\'Final decision: <final decision>\\'.\\n'\n\n        prompt2 += att_info\n\n        return prompt1, assist1, prompt2\n\n    def step(self, action):\n        \n        # Step the wrapped environment and capture all returned values\n        obs, dqn_reward, done, truncated, info = self.env.step(action)\n        \n        if np.random.rand() <= 0.33:\n        \n            prompt1, assist1, prompt2 = self.prompt_design(obs)\n\n            action_llm = llm_action(prompt1, assist1, prompt2, self.prev_action).strip().split('.')[0]\n\n            l_acts  = 0\n            if 'LANE_LEFT' in action_llm:\n                l_acts += 1\n                act     = 'LANE_LEFT'\n                llm_act = 0\n            if 'IDLE' in action_llm:\n                l_acts += 1\n                act     = 'IDLE'\n                llm_act = 1\n            if 'LANE_RIGHT' in action_llm:\n                l_acts += 1\n                act     = 'LANE_RIGHT'\n                llm_act = 2\n            if 'FASTER' in action_llm:\n                l_acts += 1\n                act     = 'FASTER'\n                llm_act = 3\n            if 'SLOWER' in action_llm:\n                l_acts += 1\n                act     = 'SLOWER'\n                llm_act = 4\n\n            if l_acts == 1:\n                if llm_act == action:\n                    llm_reward = 1\n                else:\n                    llm_reward = 0\n                comb_reward = 0.7*dqn_reward + 0.3*llm_reward\n            else:\n                comb_reward = dqn_reward\n        else:\n            comb_reward = dqn_reward\n            \n        self.prev_action = action_dict[action]\n            \n        Reward = 1 / (1 + np.exp(-comb_reward))\n        \n        return obs, Reward, done, truncated, info\n\n    def reset(self, **kwargs):\n        obs = self.env.reset(**kwargs)\n        return obs  # Make sure to return the observation","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import DummyVecEnv\nenv = MyHighwayEnv()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DQN('MlpPolicy', env,\n            policy_kwargs=dict(net_arch=[256, 256]),\n            learning_rate=5e-4,\n            buffer_size=15000,\n            learning_starts=200,\n            batch_size=32,\n            gamma=0.8,\n            train_freq=1,\n            gradient_steps=1,\n            target_update_interval=50,\n            exploration_fraction=0.7,\n            verbose=1)\n\nmodel.learn(int(2e4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('models/llm_model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gymnasium.wrappers import RecordVideo\n# base setting\nvehicleCount = 10\n\n# environment setting\nconfig = {\n    \"observation\": {\n        \"type\": \"Kinematics\",\n        \"features\": [\"presence\", \"x\", \"y\", \"vx\", \"vy\"],\n        \"absolute\": True,\n        \"normalize\": True,\n        \"vehicles_count\": vehicleCount,\n        \"see_behind\": True,\n    },\n    \"action\": {\n        \"type\": \"DiscreteMetaAction\",\n        \"target_speeds\": np.linspace(0, 32, 9),\n    },\n    \"duration\": 40,\n    \"vehicles_density\": 2,\n    \"show_trajectories\": True,\n    \"render_agent\": True,\n}\n\n\nenv = gym.make('highway-v0', render_mode='rgb_array')\nenv.configure(config)\nenv = record_videos(env)\nfor episode in trange(3, desc='Test episodes'):\n    (obs, info), done, truncated = env.reset(), False, False\n    while not (done or truncated):\n        action, _ = model.predict(obs, deterministic=True)\n        obs, reward, done, truncated, info = env.step(int(action))\nenv.close()\nshow_videos()","metadata":{},"execution_count":null,"outputs":[]}]}