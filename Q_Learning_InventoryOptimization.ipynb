{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGmI6aRysvVk",
        "outputId": "1fcc4a20-335d-4e9c-9347-817e58955011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "State: Low, Action: Reorder\n",
            "State: Medium, Action: Reorder\n",
            "State: High, Action: DoNotOrder\n",
            "\n",
            "Q-Table:\n",
            "State: Low, Q-values: {'Reorder': 11.339793985120997, 'DoNotOrder': 3.092250843855944}\n",
            "State: Medium, Q-values: {'Reorder': 6.959861864361998, 'DoNotOrder': 6.35397452872472}\n",
            "State: High, Q-values: {'Reorder': 5.707805618081022, 'DoNotOrder': 7.558512575501913}\n",
            "State: Shutdown, Q-values: {'Reorder': 0, 'DoNotOrder': 0}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define states\n",
        "states = ['Low', 'Medium', 'High', 'Shutdown']\n",
        "\n",
        "# Define actions\n",
        "actions = ['Reorder', 'DoNotOrder']\n",
        "\n",
        "# Define transition probabilities and rewards\n",
        "# Assuming transitions and rewards for simplicity. In practice, these would be based on the business context.\n",
        "transition_probabilities = {\n",
        "    'Low': {'Reorder': {'Low': 0.3, 'Medium': 0.4, 'High': 0.1, 'Shutdown': 0.2},\n",
        "            'DoNotOrder': {'Low': 0.5, 'Medium': 0.1, 'High': 0.0, 'Shutdown': 0.4}},\n",
        "    'Medium': {'Reorder': {'Low': 0.1, 'Medium': 0.2, 'High': 0.6, 'Shutdown': 0.1},\n",
        "               'DoNotOrder': {'Low': 0.4, 'Medium': 0.4, 'High': 0.0, 'Shutdown': 0.2}},\n",
        "    'High': {'Reorder': {'Low': 0.0, 'Medium': 0.2, 'High': 0.8, 'Shutdown': 0.0},\n",
        "             'DoNotOrder': {'Low': 0.2, 'Medium': 0.5, 'High': 0.2, 'Shutdown': 0.1}},\n",
        "    'Shutdown': {'Reorder': {'Low': 0.0, 'Medium': 0.0, 'High': 0.0, 'Shutdown': 1.0},\n",
        "                 'DoNotOrder': {'Low': 0.0, 'Medium': 0.0, 'High': 0.0, 'Shutdown': 1.0}},\n",
        "}\n",
        "\n",
        "rewards = {\n",
        "    'Low': {'Reorder': 5, 'DoNotOrder': -5},\n",
        "    'Medium': {'Reorder': 1, 'DoNotOrder': 2},\n",
        "    'High': {'Reorder': -2, 'DoNotOrder': 0},\n",
        "    'Shutdown': {'Reorder': 0, 'DoNotOrder': 0}\n",
        "}\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = {}\n",
        "for state in states:\n",
        "    Q[state] = {}\n",
        "    for action in actions:\n",
        "        Q[state][action] = 0\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.99  # Initial exploration rate\n",
        "min_epsilon = 0.01  # Minimum exploration rate\n",
        "decay_rate = (epsilon - min_epsilon) / 1000  # Decay rate\n",
        "\n",
        "# Number of episodes\n",
        "episodes = 1000\n",
        "\n",
        "# Q-learning algorithm\n",
        "for episode in range(episodes):\n",
        "    state = random.choice(states[:-1])  # Start with a random non-terminal state\n",
        "\n",
        "    while state != 'Shutdown':\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)  # Explore action space\n",
        "        else:\n",
        "            action = max(Q[state], key=Q[state].get)  # Exploit learned values\n",
        "\n",
        "        # Get next state based on transition probabilities\n",
        "        next_state = np.random.choice(states, p=[transition_probabilities[state][action][s] for s in states])\n",
        "\n",
        "        # Get reward for the current state-action pair\n",
        "        reward = rewards[state][action]\n",
        "\n",
        "        # Update Q-value\n",
        "        Q[state][action] = Q[state][action] + alpha * (reward + gamma * max(Q[next_state].values()) - Q[state][action])\n",
        "\n",
        "        # Transition to the next state\n",
        "        state = next_state\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(min_epsilon, epsilon - decay_rate)\n",
        "\n",
        "# Derive the optimal policy\n",
        "policy = {}\n",
        "for state in states[:-1]:  # Exclude terminal state\n",
        "    policy[state] = max(Q[state], key=Q[state].get)\n",
        "\n",
        "print(\"Optimal Policy:\")\n",
        "for state in policy:\n",
        "    print(f\"State: {state}, Action: {policy[state]}\")\n",
        "\n",
        "print(\"\\nQ-Table:\")\n",
        "for state in Q:\n",
        "    print(f\"State: {state}, Q-values: {Q[state]}\")"
      ]
    }
  ]
}